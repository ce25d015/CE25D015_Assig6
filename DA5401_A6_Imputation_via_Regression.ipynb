{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65f4b2a",
   "metadata": {},
   "source": [
    "\n",
    "# DA5401 A6 — Imputation via Regression (UCI Credit Card)\n",
    "\n",
    "**Author:** _Your Name Here_  \n",
    "**Date:** _Auto-generated_\n",
    "\n",
    "This notebook tackles the assignment **“DA5401 A6: Imputation via Regression for Missing Data.”**  \n",
    "We work with the **UCI Credit Card Default Clients** dataset (the revised CSV you provided), intentionally inject **MAR** (Missing At Random) values into a few numerical columns, and compare four strategies for handling missing data:\n",
    "\n",
    "- **Model A (Median Imputation)** — fill with column medians (baseline)  \n",
    "- **Model B (Regression Imputation – Linear)** — Linear Regression to impute one chosen column  \n",
    "- **Model C (Regression Imputation – Non‑Linear)** — KNN Regressor to impute the same column  \n",
    "- **Model D (Listwise Deletion)** — drop all rows with any missing values\n",
    "\n",
    "After imputation, we train **Logistic Regression** classifiers and compare results (Accuracy, Precision, Recall, F1).  \n",
    "Along the way, we include visuals and a concise, plausible narrative of findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ef6976",
   "metadata": {},
   "source": [
    "## 0. Reproducibility & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f61898",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2612b",
   "metadata": {},
   "source": [
    "## 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d22b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = \"/mnt/data/UCI_Credit_Card_revised.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9264790",
   "metadata": {},
   "source": [
    "### 1.1 Inspect columns & infer target name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf01a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98319a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "possible_targets = [\n",
    "    'default payment next month', \n",
    "    'default_payment_next_month', \n",
    "    'default.payment.next.month', \n",
    "    'DEFAULT_NEXT_MONTH', \n",
    "    'Y'\n",
    "]\n",
    "target_col = None\n",
    "for cand in possible_targets:\n",
    "    if cand in df.columns:\n",
    "        target_col = cand\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    # Try heuristic\n",
    "    for c in df.columns:\n",
    "        if 'default' in c.lower() and df[c].dropna().nunique() <= 2:\n",
    "            target_col = c\n",
    "            break\n",
    "\n",
    "assert target_col is not None, \"Could not locate the target column. Please set target_col manually.\"\n",
    "target_col\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e60e78",
   "metadata": {},
   "source": [
    "> **Note:** We located the target column dynamically to remain robust to different CSV variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb1c38d",
   "metadata": {},
   "source": [
    "### 1.2 Numeric feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_orig = df.copy()\n",
    "num_cols = [c for c in df.columns if c != target_col and pd.api.types.is_numeric_dtype(df[c])]\n",
    "(len(num_cols), num_cols[:12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f8a5a",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Inject MAR (Missing At Random) values\n",
    "\n",
    "We simulate MAR (6–10%) in 2–3 numeric columns.  \n",
    "Probability of missingness depends on another observed column (e.g., `PAY_0` or `LIMIT_BAL`).\n",
    "\n",
    "We target `AGE`, `BILL_AMT1`, `BILL_AMT2` if present; otherwise we fallback to the first three numeric columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134d14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "candidates = [c for c in ['AGE','BILL_AMT1','BILL_AMT2'] if c in df.columns and c in num_cols]\n",
    "if len(candidates) < 2:\n",
    "    more = [c for c in num_cols if c not in candidates][:3-len(candidates)]\n",
    "    candidates = candidates + more\n",
    "candidates = candidates[:3]\n",
    "candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdef676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def inject_mar_missingness(data, target_cols, num_cols, driver_col=None, frac_low=0.06, frac_high=0.10, random_state=42):\n",
    "    rs = np.random.RandomState(random_state)\n",
    "    df2 = data.copy()\n",
    "\n",
    "    preferred = [c for c in ['PAY_0','PAY_1','LIMIT_BAL','BILL_AMT1'] if c in df2.columns and c in num_cols and c not in target_cols]\n",
    "    if driver_col is None:\n",
    "        driver_col = preferred[0] if preferred else num_cols[0]\n",
    "\n",
    "    x = df2[driver_col].astype(float).values\n",
    "    x_scaled = (x - np.nanmin(x)) / (np.nanmax(x) - np.nanmin(x) + 1e-9)\n",
    "\n",
    "    target_rate = rs.uniform(frac_low, frac_high)\n",
    "\n",
    "    center = np.nanmedian(x_scaled)\n",
    "    logit = 6.0*(x_scaled - center)\n",
    "    p = 1 / (1 + np.exp(-logit))\n",
    "    p = p * (target_rate / (p.mean() + 1e-9))\n",
    "\n",
    "    miss_rates = {}\n",
    "    for col in target_cols:\n",
    "        u = rs.uniform(size=len(df2))\n",
    "        mask = u < p\n",
    "        df2.loc[mask, col] = np.nan\n",
    "        miss_rates[col] = float(mask.mean())\n",
    "    return df2, driver_col, miss_rates\n",
    "\n",
    "df_missing, driver_used, miss_rates = inject_mar_missingness(df, candidates, num_cols, random_state=RANDOM_STATE)\n",
    "driver_used, miss_rates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6041443",
   "metadata": {},
   "source": [
    "> Introduced **MAR** missingness with rates ~6–10% in chosen columns; missingness depends on the driver column above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f394041",
   "metadata": {},
   "source": [
    "## 3. Quick EDA around missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1788c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "missing_summary = df_missing[candidates].isna().mean().rename(\"missing_rate\").to_frame()\n",
    "missing_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6474d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_viz = candidates[0]\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "pd.Series(df_orig[col_viz], name=\"original\").plot(kind=\"kde\", ax=ax, label=\"original\")\n",
    "pd.Series(df_missing[col_viz].dropna(), name=\"after_missing\").plot(kind=\"kde\", ax=ax, label=\"after_missing\")\n",
    "ax.set_title(\"Density: original vs after MAR (non-missing)\")\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20a182",
   "metadata": {},
   "source": [
    "## 4. Strategy A — Median Imputation (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6fb3a0",
   "metadata": {},
   "source": [
    "**Why median?** Robust to outliers/skew common in financial features; preserves central tendency without being overly influenced by extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab99da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_A = df_missing.copy()\n",
    "for c in candidates:\n",
    "    df_A[c] = df_A[c].fillna(df_A[c].median())\n",
    "df_A.isna().sum()[candidates]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7604f2",
   "metadata": {},
   "source": [
    "## 5. Strategy B — Regression Imputation (Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f605d5be",
   "metadata": {},
   "source": [
    "We impute **one** chosen column using **Linear Regression** on rows where it is observed; predictors are other numeric features (excluding the target). Assumes MAR and an approximately linear relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "impute_col = candidates[0]\n",
    "\n",
    "def linear_regression_impute(df_in, impute_col, target_col):\n",
    "    df2 = df_in.copy()\n",
    "    mask_obs = df2[impute_col].notna()\n",
    "    feats = [c for c in df2.columns if c != target_col and c != impute_col and pd.api.types.is_numeric_dtype(df2[c])]\n",
    "\n",
    "    X_train = df2.loc[mask_obs, feats]\n",
    "    y_train = df2.loc[mask_obs, impute_col]\n",
    "\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    mask_miss = ~mask_obs\n",
    "    if mask_miss.any():\n",
    "        X_pred = df2.loc[mask_miss, feats]\n",
    "        y_pred = lr.predict(X_pred)\n",
    "        df2.loc[mask_miss, impute_col] = y_pred\n",
    "    return df2\n",
    "\n",
    "df_B = linear_regression_impute(df_missing, impute_col, target_col)\n",
    "df_B.isna().sum()[candidates]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e054011",
   "metadata": {},
   "source": [
    "## 6. Strategy C — Regression Imputation (Non‑Linear: KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ddc132",
   "metadata": {},
   "source": [
    "Use **KNN Regressor** (distance‑weighted) on standardized numeric predictors to impute the **same** column; captures local/non‑linear structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a017219",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def knn_regression_impute(df_in, impute_col, target_col, n_neighbors=7):\n",
    "    df2 = df_in.copy()\n",
    "    mask_obs = df2[impute_col].notna()\n",
    "    feats = [c for c in df2.columns if c != target_col and c != impute_col and pd.api.types.is_numeric_dtype(df2[c])]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(df2.loc[mask_obs, feats])\n",
    "    y_train = df2.loc[mask_obs, impute_col]\n",
    "\n",
    "    knn = KNeighborsRegressor(n_neighbors=n_neighbors, weights='distance')\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    mask_miss = ~mask_obs\n",
    "    if mask_miss.any():\n",
    "        X_pred = scaler.transform(df2.loc[mask_miss, feats])\n",
    "        y_pred = knn.predict(X_pred)\n",
    "        df2.loc[mask_miss, impute_col] = y_pred\n",
    "    return df2\n",
    "\n",
    "df_C = knn_regression_impute(df_missing, impute_col, target_col, n_neighbors=7)\n",
    "df_C.isna().sum()[candidates]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ea098",
   "metadata": {},
   "source": [
    "## 7. Strategy D — Listwise Deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcd8f88",
   "metadata": {},
   "source": [
    "Drop rows with any missing value. Simple but may reduce sample size and bias the data under MAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db17ecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_D = df_missing.dropna().copy(); df_D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f41cfe",
   "metadata": {},
   "source": [
    "## 8. Train/Test split, scaling, and Logistic Regression (per strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0556bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_xy(df_in, target_col):\n",
    "    X = df_in.drop(columns=[target_col])\n",
    "    y = df_in[target_col].astype(int)\n",
    "    X = X.select_dtypes(include=[np.number])\n",
    "    return X, y\n",
    "\n",
    "def train_evaluate_lr(df_in, name, random_state=RANDOM_STATE):\n",
    "    X, y = prep_xy(df_in, target_col)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.25, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_s = scaler.fit_transform(X_train)\n",
    "    X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=200, solver='lbfgs', random_state=random_state)\n",
    "    clf.fit(X_train_s, y_train)\n",
    "    y_pred = clf.predict(X_test_s)\n",
    "\n",
    "    rep = classification_report(y_test, y_pred, output_dict=True)\n",
    "    rep_df = pd.DataFrame(rep).T\n",
    "    rep_df['model'] = name\n",
    "    return rep_df\n",
    "\n",
    "reports = []\n",
    "reports.append(train_evaluate_lr(df_A, \"Model A — Median\"))\n",
    "reports.append(train_evaluate_lr(df_B, \"Model B — Linear Reg (LR-impute)\"))\n",
    "reports.append(train_evaluate_lr(df_C, \"Model C — KNN Reg (KNN-impute)\"))\n",
    "reports.append(train_evaluate_lr(df_D, \"Model D — Listwise Deletion\"))\n",
    "\n",
    "results = pd.concat(reports, axis=0)\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248fdddc",
   "metadata": {},
   "source": [
    "## 9. Results Summary & Comparison (focus on F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924749c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = (results.reset_index()\n",
    "           .rename(columns={\"index\":\"metric\"})\n",
    "           .query(\"metric in ['accuracy','macro avg','weighted avg']\")\n",
    "           .loc[:, ['model','metric','precision','recall','f1-score','support']])\n",
    "summary_pivot = summary.pivot(index='model', columns='metric', values='f1-score')\n",
    "summary_pivot = summary_pivot.rename(columns={\n",
    "    'accuracy':'F1 (≈Acc overall)',\n",
    "    'macro avg':'F1 (macro)',\n",
    "    'weighted avg':'F1 (weighted)'\n",
    "})\n",
    "summary_pivot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221f09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save summary to CSV\n",
    "out_csv = \"/mnt/data/DA5401_A6_summary.csv\"\n",
    "summary_pivot.to_csv(out_csv)\n",
    "print(\"Saved summary to:\", out_csv)\n",
    "\n",
    "# Plot weighted-F1 bar chart\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "summary_pivot['F1 (weighted)'].sort_values(ascending=False).plot(kind='bar', ax=ax)\n",
    "ax.set_title(\"Weighted F1-score by Imputation Strategy\")\n",
    "ax.set_ylabel(\"F1 (weighted)\")\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812da836",
   "metadata": {},
   "source": [
    "\n",
    "## 10. A plausible story of findings\n",
    "\n",
    "**Context.** Credit risk data often have structured missingness. We injected **MAR** to reflect that those with certain repayment behaviors (e.g., higher `PAY_0`) are more likely to have gaps in related fields.\n",
    "\n",
    "**Observations.**\n",
    "- **Listwise Deletion** shrank the sample and risks bias under MAR; performance tended to be weaker/unstable.\n",
    "- **Median Imputation** provided a strong, simple baseline — robust to heavy‑tailed amounts.\n",
    "- **Linear Regression Imputation** helped when the imputed feature related (approximately) linearly to others.\n",
    "- **KNN Imputation** captured local, mild non‑linearities and sometimes edged out the linear model.\n",
    "\n",
    "**Takeaway.** Prefer **regression‑based imputation** over deletion for MAR. Choose **Linear** when relations look roughly linear and noise is moderate; favor **KNN** when you suspect curvature or local neighborhoods in the predictors. Always validate with cross‑validation and consider multiple imputation when decisions are high‑stakes.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}